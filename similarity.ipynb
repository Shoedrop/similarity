{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Text Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #1: Importing neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step #2: Defining the SimilarityCalculator class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimilarityCalculator:\n",
    "    def __init__(self, corpus_directory, search_directory): # O(1)\n",
    "        \"\"\"\n",
    "        Initializes a SimilarityCalculator object.\n",
    "        Parameters:\n",
    "        - corpus_directory (str): The directory path of the corpus.\n",
    "        - search_directory (str): The directory path of the search documents.\n",
    "        \"\"\"\n",
    "        self.corpus_directory = corpus_directory\n",
    "        self.search_directory = search_directory\n",
    "        self.unique_words_corpus = {}\n",
    "        self.unique_words_search = {}\n",
    "        self.vectors = {}\n",
    "\n",
    "    def clean_word(self, word): # O(n)\n",
    "        \"\"\"\n",
    "        Cleans a word by removing non-alphabetic characters and converting it to lowercase.\n",
    "        Parameters:\n",
    "        - word (str): The word to be cleaned.\n",
    "        Returns:\n",
    "        - str: The cleaned word.\n",
    "        \"\"\"\n",
    "        # We will use the following regular expression to remove non-alphabetic characters: [^a-zA-Z]\n",
    "        return re.sub(\"[^a-zA-Z]\", \"\", word).lower()\n",
    "\n",
    "    def get_unique_words(self, target_directory): # O(n*m)\n",
    "        \"\"\"\n",
    "        Loads the file into a dictionary of unique words.\n",
    "        Parameters:\n",
    "        - target_directory (str): The directory path of the target documents.\n",
    "        \"\"\"\n",
    "        for file_name in os.listdir(target_directory):\n",
    "            if file_name.endswith(\".txt\"):\n",
    "                unique_words = set()\n",
    "                try:\n",
    "                    with open(\n",
    "                        os.path.join(target_directory, file_name), \"r\", encoding=\"utf-8\"\n",
    "                    ) as file:\n",
    "                        for line in file:\n",
    "                            for word in line.split():\n",
    "                                unique_words.add(self.clean_word(word))\n",
    "                except UnicodeDecodeError:\n",
    "                    with open(\n",
    "                        os.path.join(target_directory, file_name),\n",
    "                        \"r\",\n",
    "                        encoding=\"ISO-8859-1\",\n",
    "                    ) as file:\n",
    "                        for line in file:\n",
    "                            for word in line.split():\n",
    "                                unique_words.add(self.clean_word(word))\n",
    "                if target_directory == self.search_directory:\n",
    "                    self.unique_words_search[\"search\"] = unique_words\n",
    "                else:\n",
    "                    self.unique_words_corpus[file_name] = unique_words\n",
    "            else:\n",
    "                print(\n",
    "                    f\"File {file_name} is not a text file and will therefore not be processed.\"\n",
    "                )\n",
    "\n",
    "    def vectorize_documents(self): # O(n*m)\n",
    "        \"\"\"\n",
    "        Vectorizes the documents based on the unique words.\n",
    "        \"\"\"\n",
    "        # Vectorize corpus\n",
    "        for file_name, unique_words in self.unique_words_corpus.items():\n",
    "            vector = [\n",
    "                1 if word in unique_words else 0\n",
    "                for word in self.unique_words_search[\"search\"]\n",
    "            ]\n",
    "            self.vectors[file_name] = vector\n",
    "\n",
    "        # Vectorize search\n",
    "        for file_name, unique_words in self.unique_words_search.items():\n",
    "            vector = [\n",
    "                1 if word in unique_words else 0\n",
    "                for word in self.unique_words_search[\"search\"]\n",
    "            ]\n",
    "            self.vectors[file_name] = vector\n",
    "\n",
    "    def calculate_similarity(self, vec1, vec2, measure): # O(n)\n",
    "        \"\"\"\n",
    "        Calculates the similarity between two vectors.\n",
    "        Parameters:\n",
    "        - vec1 (list): The first vector.\n",
    "        - vec2 (list): The second vector.\n",
    "        - measure (str): The similarity measure to be used. Possible values: 'dot', 'euclidean', 'cosine'.\n",
    "        Returns:\n",
    "        - float: The similarity score.\n",
    "        \"\"\"\n",
    "        if measure == \"dot\":\n",
    "            return np.dot(vec1, vec2)\n",
    "        elif measure == \"euclidean\":\n",
    "            return 1 / (1 + np.linalg.norm(np.array(vec1) - np.array(vec2)))\n",
    "        elif measure == \"cosine\":\n",
    "            return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def calculate_similarities(self, measure): # O(n)\n",
    "        \"\"\"\n",
    "        Calculates the similarity between the search document and the corpus documents.\n",
    "        Parameters:\n",
    "        - measure (str): The similarity measure to be used. Possible values: 'dot', 'euclidean', 'cosine'.\n",
    "        Returns:\n",
    "        - dict: A dictionary containing the similarity scores for each corpus document.\n",
    "        \"\"\"\n",
    "        dic_similarity = {}\n",
    "        for fileName, vector in self.vectors.items():\n",
    "            if fileName != \"search\":\n",
    "                similarity = self.calculate_similarity(\n",
    "                    self.vectors[\"search\"], vector, measure\n",
    "                )\n",
    "                dic_similarity[fileName] = similarity\n",
    "        return dic_similarity\n",
    "\n",
    "    def checkPlagiarism(self): # O(n*m)\n",
    "        \"\"\"\n",
    "        Checks if the search document is plagiarized.\n",
    "        Returns:\n",
    "        - df: A dataframe containing the similarity scores for each corpus document.\n",
    "        \"\"\"\n",
    "        self.get_unique_words(self.corpus_directory)\n",
    "        self.get_unique_words(self.search_directory)\n",
    "        self.vectorize_documents()\n",
    "        self.results_dot = self.calculate_similarities(\"dot\")\n",
    "        self.results_euclidean = self.calculate_similarities(\"euclidean\")\n",
    "        self.results_cosine = self.calculate_similarities(\"cosine\")\n",
    "\n",
    "        self.results_df = pd.DataFrame(\n",
    "            {\n",
    "                \"dot\": self.results_dot,\n",
    "                \"euclidean\": self.results_euclidean,\n",
    "                \"cosine\": self.results_cosine,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Cosine similarity is seen as the most accurate measure for plagiarism detection. Therefore, the df is sorted by cosine similarity.\n",
    "        self.results_df = self.results_df.sort_values(by=\"cosine\", ascending=False)\n",
    "\n",
    "        return f\"Similarity of search document at path '{self.search_directory}' with corpus documents at path '{self.corpus_directory}':\\n{self.results_df}\\nHigher values indicate higher similarity.\"\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"corpusDirectory={self.corpus_directory} \\n searchDirectory={self.search_directory} \\n {self.results_df}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define paths, create class instance and run the similarity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this code is running on a Windows machine, the paths can be set as follows:\n",
    "# corpus_directory = \"..\\\\question3\\\\corpusFiles\"\n",
    "# search_directory = \"..\\\\question3\\\\searchFiles\"\n",
    "\n",
    "# If this code is running on a Mac/Linux machine, the paths can be set as follows:\n",
    "corpus_directory = \"../question3/corpusFiles\"\n",
    "search_directory = \"../question3/searchFiles\"\n",
    "\n",
    "test_exam = SimilarityCalculator(corpus_directory, search_directory)\n",
    "results = test_exam.checkPlagiarism()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
